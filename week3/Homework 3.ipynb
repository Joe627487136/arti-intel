{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "## Theory Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Consider a CNN that has**\n",
    "1. Input of 14x14 with 30 channels.\n",
    "2. A convolution layer C with 12 filters, each of size 4x4. The convolution zero padding is 1 and stride is 2, followed by a ReLU activation.\n",
    "3. A max pooling layer P that is applied over each of the C's output feature map, using 3x3 receptive field and stride 2.\n",
    "\n",
    "**What is the total size of C's output feature map?**\n",
    "\n",
    "Convolution output size is given as\n",
    "\n",
    "$$ O_{size} = ceil(( M + 2r - ksize + 1) / k) $$\n",
    "\n",
    "where $M$ is the size of the concerned dimension, $r$ is padding and $k$ is stride, and $ksize$ is kernel size of the corresponding dimension. Therefore, \n",
    "\n",
    "$$ O_{size} = ceil(( 14 + 2*1 - 4 + 1) / 2) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Osize = math.ceil((14 + 2*1 - 4 + 1)/2)\n",
    "print(Osize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the total size of C's output feature map is 7x7.\n",
    "\n",
    "\n",
    "**What is the total size of P's output feature map?**\n",
    "\n",
    "Same formula, just without the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "Osize = math.ceil((7 - 3 + 1)/2)\n",
    "print(Osize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the total size of O's output feature map is 3x3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compute the overhead of the above CNN in terms of floating point operation\n",
    "(FLOP). FLOP can be used to measure computer’s performance. A decent processor nowadays\n",
    "can perform in Giga-FLOPS, that means billions of FLOP per second. Assume the inputs are all\n",
    "scalars (we have 14 × 14 × 30 scalars as input), we have the computational cost of:\n",
    "1. 1 FLOP for a single scalar multiplication xi\n",
    "· xj\n",
    "2. 1 FLOP for a single scalar addition xi + xj\n",
    "3. (n − 1) FLOPs for a max operation over n items: max{x1, ..., xn}\n",
    "\n",
    "**How many FLOPs layer C and P cost in total to do one forward pass?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cell of each filter in C's output feature map, there must necessarily be $ksize^2$ number of _multiplication_ done for each of the input _channel_ (for this 2d kernel).\n",
    "\n",
    "$$ C_{mul} = O_{size}^2 * 30 * ksize^2 * filters$$\n",
    "$$ C_{mul} = 7^2 * 30 * 4^2 * 12$$\n",
    "$$ C_{mul} = 282240$$\n",
    "\n",
    "For each cell of each filter in C's output feature map, there must also necessarily be $(ksize^2 - 1)$ number of _addition_ (for this 2d kernel) to sum up all multiplication that is done for that output cell, for each of the input _channel_ (????).\n",
    "\n",
    "$$ C_{add} = O_{size}^2 * (ksize^2 - 1) * filters * channel$$\n",
    "$$ C_{add} = 7^2 * (4^2 - 1) * 12 * 30$$\n",
    "$$ C_{add} = 264600$$\n",
    "\n",
    "For each cell of P's output feature map, there must be a _max operation_ operating on $ksize^2$ cells of the previous map.\n",
    "\n",
    "$$ P_{max} = O_{size}^2 * C_{channel} * (ksize^2 - 1) $$\n",
    "\n",
    "$$ P_{max} = 3^2 * 12 * (3^2 - 1)$$\n",
    "\n",
    "$$ P_{max} = 864$$\n",
    "\n",
    "so the total FLOP is,\n",
    "\n",
    "$$ T_{FLOP} = C_{mul} + C_{add} + P_{max} $$\n",
    "\n",
    "$$ T_{FLOP} = 282240 + 264600 + 864 $$\n",
    "\n",
    "$$ T_{FLOP} = 547704 $$\n",
    "\n",
    "***\n",
    "\n",
    "..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2 Refer to the neural network at _figure 1_ with input $x ∈ R^1$. The activation function for\n",
    "$z_1$, $z_2$, and $z_3$ is the sigmoid function: $\\frac{1}{1+e^{-w \\cdot x}}$,**\n",
    "\n",
    "$$ h(x) = \\frac{1}{1+e^{-x}} \\quad\\quad (1)$$\n",
    "\n",
    "$$ z_1 = h(x \\cdot w_{(x,1)}) \\quad\\quad (2)$$\n",
    "\n",
    "$$ z_2 = h(z_1 \\cdot w_{(1,2)}) \\quad\\quad (3)$$\n",
    "\n",
    "$$ z_3 = h(z_1 \\cdot w_{(1,3)}) \\quad\\quad (4)$$\n",
    "\n",
    "For the error E, instead of using the softmax function we learned in class, we use the quadratic\n",
    "error function for regression purpose,\n",
    "\n",
    "$$ E = \\sum_{i \\space \\epsilon \\space data } ((z_2 − y_{2i})^2 + (z_3 − y_{3i})^2)$$\n",
    "\n",
    "[**6p**] Write down an expression for the gradients of all three weights: $ \\frac{∂E}{∂w(x,1)}, \\frac{∂E}{∂w(1,2)},\\frac{∂E}{∂w(1,3)}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going backwards through the network, \n",
    "\n",
    "$$ \\newcommand{\\partial}[1]{∂ #1} $$\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,3)}} = \\sum_{i \\space \\epsilon \\space data } \\frac{\\partial{(z_2 − y_{2i})^2}}{\\partial{w(1,3)}} + \\frac{\\partial{(z_3 − y_{3i})^2}}{\\partial{w(1,3)}}$$\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,3)}} = \\sum_{i \\space \\epsilon \\space data } \\frac{\\partial{(z_3 − y_{3i})^2}}{\\partial{w(1,3)}}$$\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,3)}} = \\sum_{i \\space \\epsilon \\space data } 2(z_3 − y_{3i}) \\cdot \\frac{\\partial{z_3}}{\\partial{w(1,3)}}$$\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,3)}} = \\sum_{i \\space \\epsilon \\space data } 2(z_3 − y_{3i}) \\cdot \\frac{\\partial{h(z_1 \\cdot w_{(1,3)})}}{\\partial{w(1,3)}}$$\n",
    "\n",
    "$$ since \\quad \\quad \\frac{\\partial{h(x)}}{\\partial(x)} = h(x) \\cdot (1 - h(x)), \\quad \\quad \\space <sigmoid>$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,3)}} = \\sum_{i \\space \\epsilon \\space data } 2(z_3 − y_{3i}) \\cdot h(z_1 \\cdot w_{(1,3)}) \\cdot (1-h(z_1 \\cdot w_{(1,3)})) \\cdot \\frac{\\partial{(z_1 \\cdot w_{(1,3)})}}{\\partial{w(1,3)}}$$\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,3)}} = \\sum_{i \\space \\epsilon \\space data } 2(z_3 − y_{3i}) \\cdot h(z_1 \\cdot w_{(1,3)}) \\cdot (1-h(z_1 \\cdot w_{(1,3)})) \\cdot z_1 $$\n",
    "\n",
    "Likewise, \n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(1,2)}} = \\sum_{i \\space \\epsilon \\space data } 2(z_2 − y_{2i}) \\cdot h(z_1 \\cdot w_{(1,2)}) \\cdot (1-h(z_1 \\cdot w_{(1,2)})) \\cdot z_1 $$\n",
    "\n",
    "As for $w_{(x,1)}$,\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(x,1)}} = \\sum_{i \\space \\epsilon \\space data } \\frac{\\partial{(z_2 − y_{2i})^2}}{\\partial{w(x,1)}} + \\frac{\\partial{(z_3 − y_{3i})^2}}{\\partial{w(x,1)}}$$\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w(x,1)}} = \\sum_{a \\space \\epsilon \\space (2,3)} \\sum_{i \\space \\epsilon \\space data } \\frac{\\partial{(z_a − y_{ai})^2}}{\\partial{w(x,1)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d35c46f8d1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import getimagenetclasses as GINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982 n10148035 groom, bridegroom\n"
     ]
    }
   ],
   "source": [
    "# GINC part - what are these functions for?\n",
    "ind2syn, syn2ind, syn2desc  = GINC.parsesynsetwords('synset_words.txt')\n",
    "\n",
    "src_xml_regex = r'bbox_val/val/ILSVRC2012_val_{}.xml'\n",
    "val_00049999 = src_xml_regex.format('00049999')\n",
    "\n",
    "label, firstname = GINC.parseclasslabel(val_00049999, syn2ind)\n",
    "print(label, firstname, syn2desc[ind2syn[label]])\n",
    "\n",
    "img_file_regex = r'..\\datasets\\imagenet_first2500\\imagenet2500\\imagespart\\ILSVRC2012_val_{}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
