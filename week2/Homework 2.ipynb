{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Gede Ria Ghosalya (1001841)_\n",
    "\n",
    "## Homework 2\n",
    "\n",
    "** Q1. Find any separating hyperplane equation for these three sample points: **\n",
    "\n",
    "$x^{(1)} = (−2, 2), \\space y^{(1)} = −1, \\quad x^{(2)} = (4, 0), \\space y^{(2)} = −1, \\quad and \\space x^{(3)} = (−2, −3), \\space y^{(3)} = +1 $. \n",
    "\n",
    "Draw (by hand) or plot (using Python, see matplotlib) the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the 3 points, it is possible to separate by the following equation \n",
    "\n",
    "$$ x_2 = -1 $$\n",
    "\n",
    "with the points lying as follows,\n",
    "\n",
    "$$ x^{(1)}_2 = 2 > -1, \\quad y^{(1)} = -1 $$\n",
    "$$ x^{(2)}_2 = 0 > -1, \\quad y^{(2)} = -1 $$\n",
    "$$ x^{(3)}_2 = -3 < -1, \\quad y^{(3)} = +1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e27e9b1b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x_negative = np.asarray([[-2, 2],[4,0]])\n",
    "x_positive = np.asarray([[-2, -3]])\n",
    "# y = np.ndarray([-1, -1, 1])\n",
    "\n",
    "# fig = plt.figure()\n",
    "ax = plt.figure().add_subplot(111)\n",
    "plt.plot(x_positive[:,0], x_positive[:,1], '.b')\n",
    "plt.plot(x_negative[:,0], x_negative[:,1], '.r')\n",
    "\n",
    "# for the hyperplane\n",
    "line_points = np.asarray([[-3, -1], [5, -1]])\n",
    "line = Line2D(line_points[:,0],line_points[:,1], color='y')\n",
    "ax.add_line(line)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q2. Find by hand the value of optimum weights $ \\hat w $ and bias  $ \\hat b $ using linear regression for\n",
    "the four following sample points: **\n",
    "\n",
    "$$ x^{(1)} = (1, 0), \\space y^{(1)} = +1, \\quad x^{(2)} = (2, 3), \\space y^{(2)} = +1, \\quad x^{(3)} = (3, 4), \\space y^{(3)} =\n",
    "−1, \\quad x^{(4)} = (3, 2), \\space y^{(4)} = −1. $$\n",
    "\n",
    "** Show your working. If you have to do matrix inversion, then you\n",
    "can use a program to compute the matrix inverse.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting data points to matrices (with appended 1 for X),\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "        1 & 0 & 1\\\\\n",
    "        2 & 3 & 1\\\\\n",
    "        3 & 4 & 1\\\\\n",
    "        3 & 2 & 1\\\\\n",
    "       \\end{bmatrix}, \\quad\n",
    "  Y = \\begin{bmatrix}\n",
    "       +1 \\\\\n",
    "       +1 \\\\\n",
    "       -1 \\\\\n",
    "       -1 \\\\\n",
    "      \\end{bmatrix}$$\n",
    "      \n",
    "Using closed form solution,\n",
    "\n",
    "$$ \\hat w = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y $$\n",
    "$$ \\hat w = \\left( \\begin{bmatrix}\n",
    "                    1 & 2 & 3 & 3 \\\\\n",
    "                    0 & 3 & 4 & 2 \\\\\n",
    "                    1 & 1 & 1 & 1 \\\\\n",
    "                  \\end{bmatrix} \n",
    "            \\cdot\n",
    "                \\begin{bmatrix}\n",
    "                    1 & 0 & 1\\\\\n",
    "                    2 & 3 & 1\\\\\n",
    "                    3 & 4 & 1\\\\\n",
    "                    3 & 2 & 1\\\\\n",
    "                \\end{bmatrix} \\right)^{-1} \n",
    "            \\cdot \n",
    "                \\begin{bmatrix}\n",
    "                    1 & 2 & 3 & 3 \\\\\n",
    "                    0 & 3 & 4 & 2 \\\\\n",
    "                    1 & 1 & 1 & 1 \\\\\n",
    "                \\end{bmatrix} \n",
    "            \\cdot\n",
    "                \\begin{bmatrix}\n",
    "                    +1 \\\\\n",
    "                    +1 \\\\\n",
    "                    -1 \\\\\n",
    "                    -1 \\\\\n",
    "                \\end{bmatrix}$$\n",
    "\n",
    "$$ \\hat w = \\left( \\begin{bmatrix}\n",
    "                    1+4+9+9 & 0+6+12+6 & 1+2+3+3 \\\\\n",
    "                    0+6+12+6 & 0+9+16+4 & 0+3+4+2 \\\\\n",
    "                    1+2+3+3 & 0+3+4+2 & 1+1+1+1 \\\\\n",
    "                  \\end{bmatrix}  \\right)^{-1} \n",
    "            \\cdot \n",
    "                \\begin{bmatrix}\n",
    "                    1 + 2 - 3 - 3 \\\\\n",
    "                    0 + 3 - 4 - 2 \\\\\n",
    "                    1 + 1 - 1 - 1 \\\\\n",
    "                \\end{bmatrix}$$\n",
    "                \n",
    "$$ \\hat w = \\left( \\begin{bmatrix}\n",
    "                    23 & 24 & 9 \\\\\n",
    "                    24 & 29 & 9 \\\\\n",
    "                    9 & 9 & 4 \\\\\n",
    "                  \\end{bmatrix}  \\right)^{-1} \n",
    "            \\cdot \n",
    "                \\begin{bmatrix}\n",
    "                    - 3 \\\\\n",
    "                    - 3 \\\\\n",
    "                    0 \\\\\n",
    "                \\end{bmatrix}$$\n",
    "                \n",
    "            \n",
    "              \n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.875 -0.375 -1.125]\n",
      " [-0.375  0.275  0.225]\n",
      " [-1.125  0.225  2.275]]\n"
     ]
    }
   ],
   "source": [
    "pre_w = np.asarray([[23,24,9],[24,29,9],[9,9,4]])\n",
    "pre_w_inv = np.linalg.inv(pre_w)\n",
    "print(pre_w_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat w = \\begin{bmatrix}\n",
    "                0.875 & -0.375 & -1.125 \\\\\n",
    "                -0.375 & 0.275 & 0.225 \\\\\n",
    "                -1.125 & 0.225 & 2.275 \\\\\n",
    "            \\end{bmatrix}  \n",
    "            \\cdot \n",
    "            \\begin{bmatrix}\n",
    "               - 3 \\\\\n",
    "               - 3 \\\\\n",
    "                 0 \\\\\n",
    "            \\end{bmatrix}$$\n",
    "            \n",
    "$$ \\hat w = \\begin{bmatrix}\n",
    "                -0.875*3 + 0.375*3 + 0 \\\\\n",
    "                0.375*3 - 0.275*3 + 0 \\\\\n",
    "                1.125*3 - 0.225*3 + 0 \\\\\n",
    "            \\end{bmatrix}$$\n",
    "            \n",
    "$$ \\hat w = \\begin{bmatrix}\n",
    "                - 2.625 + 1.125 \\\\\n",
    "                1.125 - 0.825 \\\\\n",
    "                3.375 - 0.675 \\\\\n",
    "            \\end{bmatrix} \n",
    "            =\n",
    "            \\begin{bmatrix}\n",
    "                -1.5 \\\\\n",
    "                0.3 \\\\\n",
    "                2.7 \\\\\n",
    "            \\end{bmatrix} \n",
    "            $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer the following question: How can you\n",
    "obtain a solution via the closed-form method (that is not by using gradients) which does not use\n",
    "an inverse matrix $ (X^T \\cdot X)^{−1} $ to obtain the weight vector $w$? **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- My Answer Here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q3. In logistic regression, we model the probability of a label $y ∈ \\{−1, 1\\}$ given a sample point $x ∈ R^2$ as, **\n",
    "\n",
    "$$ P(Y = y \\mid x) = \\frac{1}{1 + e^{−y(w·x+c)}} $$\n",
    "\n",
    "** where $c$ is the bias term. Answer the following questions.**\n",
    "\n",
    "1)  Obviously the function above alone is not linear. However, given that after we know what\n",
    "the values $w$ are (after training), prove that the decision boundary obtained from logistic\n",
    "regression is linear, i.e.: the decision boundary is a hyperplane with equation $w \\cdot X = 0$.\n",
    "Hint: Since $x$ is two-dimensional, the decision boundary is the set of $x$ such that $P(Y =\n",
    "−1 \\mid x) = P(Y = 1 \\mid x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$ P(Y = y \\mid x) = \\frac{1}{1 + e^{−y(w·x+c)}} $$\n",
    "\n",
    "For decision boundary, \n",
    "\n",
    "$$ P(Y = −1 \\mid x) = P(Y = 1 \\mid x) = 0.5 $$\n",
    "\n",
    "$$ P(Y = -1 \\mid x) = \\frac{1}{1 + e^{−(-1)(w·x+c)}} $$\n",
    "\n",
    "$$ 0.5 = \\frac{1}{1 + e^{(w·x+c)}} $$\n",
    "\n",
    "$$ 1 + e^{(w·x+c)} = 2 $$\n",
    "\n",
    "$$ e^{(w·x+c)} = 1 = e^0$$\n",
    "\n",
    "\n",
    "$$ w·x+c = 0 \\quad (linear \\space boundary)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2) Suppose we have obtained the optimum weights, $ \\hat w = (w_1, w_2) $ and bias $\\hat c$ in the case of\n",
    "2-dimensional data points. The decision boundary is supposedly a line that separate points\n",
    "with positive and negative labels, which has an equation in the form of $x_2 = mx_1 + k$.\n",
    "Obtain an expression for $m$ and $k$ in terms of $w_1$, $w_2$ and $\\hat c$. When this expression of the\n",
    "form $x_2 = mx_1 + k$ is not defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w·x+c = 0 \\quad (linear \\space boundary)$$\n",
    "\n",
    "$$ \\begin{bmatrix} w_1 & w_2 \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\hat c = 0 $$\n",
    "\n",
    "$$ w_1 x_1 + w_2 x_2 + \\hat c = 0 $$\n",
    "\n",
    "$$ w_2 x_2 = - w_1 x_1 - \\hat c $$\n",
    "\n",
    "$$ x_2 = \\frac{w_1}{w_2} x_1 - \\frac{\\hat c}{w_2} $$\n",
    "\n",
    "$$ m = -\\frac{w_1}{w_2}, \\quad k = -\\frac{\\hat c}{w_2} $$\n",
    "\n",
    "$x_2 = mx_1 + k$ is not defined when $w_2$ = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. The following is the regularized logistic regression loss function.**\n",
    "\n",
    "$$ L(w) = −\\frac{1}{m} \n",
    "        \\left[ \n",
    "            \\sum_{i=1}^{m} \n",
    "                y^{(i)} log(h_w(x^{(i)}))\n",
    "                + (1-y^{(i)})(1-log(h_w(x^{(i)}))\n",
    "        \\right]\n",
    "        + \\frac{\\lambda}{2} ||w||^2 $$\n",
    "\n",
    "**Compute the gradient of the loss with respect to w and write down the\n",
    "gradient descent update equation. Show all steps clearly.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, $y^{(i)}$ will be denoted as $y$ and so is for $x$.\n",
    "\n",
    "For regularizer $r$,\n",
    "\n",
    "$$ \\frac{dr}{dw} = \\lambda w $$\n",
    "\n",
    "For the prediction term $p_w$,\n",
    "\n",
    "$$ p_w = −\\frac{1}{m} y log(h_w(x)) + (1-y)(1-log(h_w(x)) $$\n",
    "\n",
    "$$ where \\quad h_w(x) = s(u) = \\frac{1}{1+ e^{y \\cdot u}}$$\n",
    "$$ u =  (w \\cdot x+c), \\space \\frac{du}{dw} = x $$\n",
    "\n",
    "$$ and \\quad \\frac{d \\space log(s(u))}{du} = 1 - s(u), \\space \\frac{d \\space log(1-s(u))}{du} = -s(u), $$\n",
    "\n",
    "$$ therefore \\quad \\frac{dp_w}{du} = -\\frac{1}{m} \\left( y(1-s(u)) + (-1)(1-y)(s(u) \\right) $$\n",
    "\n",
    "$$ \\frac{dp_w}{dw} = \\frac{dp_w}{du} \\cdot \\frac{du}{dw} = -\\frac{1}{m} \\left( y(1-s(u)) - (1-y)(s(u)) \\right) \\cdot  ( x) $$\n",
    "\n",
    "$$ \\frac{dp_w}{dw}  = -\\frac{1}{m} \\left( y- y \\cdot s(u) - s(u) +y \\cdot s(u) \\right) \\cdot  ( x) $$\n",
    "\n",
    "$$ \\frac{dp_w}{dw}  = -\\frac{1}{m} \\left( y - s(u) \\right) \\cdot  ( x) $$\n",
    "\n",
    "$$ \\frac{dp_w}{dw}  = -\\frac{1}{m} \\left (x) ( y - h_w(x) \\right) $$\n",
    "\n",
    "therefore,\n",
    "\n",
    "$$ \\frac{dL}{dw} = \\frac{dp_w}{dw} + \\frac{dr}{dw} $$\n",
    "\n",
    "$$ \\frac{dL}{dw} = -\\frac{1}{m} \\left(\\sum_{i=1}^{m} x^{(i)} ( y^{(i)} - h_w(x^{(i)}) \\right) + \\lambda w $$\n",
    "\n",
    "$$ w_{k+1} = w_{k} + \\frac{1}{m} \\left(\\sum_{i=1}^{m} x^{(i)} ( y^{(i)} - h_{w_k}(x^{(i)}) \\right) + \\lambda w_k $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
